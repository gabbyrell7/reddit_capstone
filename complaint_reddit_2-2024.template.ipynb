{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from spacy.pipeline import EntityRuler\n",
    "import tqdm\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy version: 3.8.5\n",
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "print(f\"spaCy version: {spacy.__version__}\")\n",
    "print(f\"CUDA available: {spacy.prefer_gpu()}\")\n",
    "# print(f\"GPU device count: {spacy.util.get_gpu_count()}\")\n",
    "\n",
    "# Load spaCy model\n",
    "spacy.prefer_gpu()\n",
    "nlp = spacy.load(\"en_core_web_sm\") \n",
    "\n",
    "# Add your custom EntityRuler\n",
    "ruler = nlp.add_pipe(\"entity_ruler\", before=\"ner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Init Variables for csv names\n",
    "year=\"2024\"\n",
    "# month=\"01\"\n",
    "cwd=os.getcwd()\n",
    "month=os.path.basename(cwd)\n",
    "# print(f\"{month}\")\n",
    "\n",
    "#special_identifier='_xfin_amt_sep_spi_ama'\n",
    "special_identifier='_PLOTS_1' #for csv output\n",
    "# Construct the directory name\n",
    "output_directory = f\"batch{special_identifier}\"\n",
    "\n",
    "services = [\n",
    "    \"Comcast\", \"Airline\", \"Healthcare\", \"Trains\", \"Banks\", \"United States\",\n",
    "    \"ER\", \"Youtube\", \"Reddit\", \"Netflix\",\n",
    "    \"Xfinity\", \"Amtrak\", \"Septa\", \"Spirit\", \"American\",\n",
    "    \"Disney\"\n",
    "]\n",
    "\n",
    "complaint_patterns = [{\"label\": \"SERVICE\", \"pattern\": service} for service in services]\n",
    "ruler.add_patterns(complaint_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_csv(f\"../../FILTERED_SUBMISSIONS_bad/RS_{year}-{month}.csv\", names=[\"id\", \"date\", \"title\", \"author\", \"url\", \"content\", \"post_id\", \"timestamp\", \"subreddit\"])\n",
    "\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original size: 486721\n",
      "Filtered size: 436221\n"
     ]
    }
   ],
   "source": [
    "# Build regex pattern for fast searching (case insensitive)\n",
    "pattern = r'|'.join(services)\n",
    "\n",
    "# Filter rows where content mentions any service\n",
    "filtered_df = df[df['content' ].str.contains(pattern, case=False, na=False) |\n",
    "                df['subreddit'].str.contains(pattern, case=False, na=False) ]\n",
    "\n",
    "print(f\"Original size: {len(df)}\")\n",
    "print(f\"Filtered size: {len(filtered_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neg_words = [\"bad\", \"terrible\", \"lazy\", \"worst\", \"awful\", \"scam\", \"horrible\", \"broken\", \"slow\"]\n",
    "neg_words = [\n",
    "            \"bad\", \"terrible\", \"lazy\", \"worst\", \"awful\", \n",
    "            \"scam\", \"horrible\", \"broken\", \"slow\", \"usless\", \n",
    "            \"sucks\", \"ripoff\", \"expensive\", \"painful\", \"crumy\",\n",
    "            \"pointless\", \"greedy\", \"fake\", \"disappointing\", \"mess\",\n",
    "            \"nightmare\"\n",
    "            ]\n",
    "\n",
    "pattern_neg = r'|'.join(neg_words)\n",
    "\n",
    "complaint_df = filtered_df[\n",
    "    filtered_df['content'].str.contains(pattern_neg, case=False, na=False)\n",
    "]\n",
    "\n",
    "# complaint_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# complaint_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created directory: batch_PLOTS_1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17b6b0c0e1ee4ca4806dac61d7e14a83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/98 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "batch_size = 5000\n",
    "max_length = 1000  # Skip posts longer than this\n",
    "skipped_posts = []\n",
    "\n",
    "# Check if the directory exists, and create it if it doesn't\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)\n",
    "    print(f\"Created directory: {output_directory}\")\n",
    "else:\n",
    "    print(f\"Directory already exists: {output_directory}\")\n",
    "\n",
    "\n",
    "for start in tqdm(range(0, len(df), batch_size)):\n",
    "    end = start + batch_size\n",
    "    batch = df.iloc[start:end].copy()\n",
    "\n",
    "    # Only keep rows with short enough content\n",
    "    batch = batch[batch['content'].str.len() < max_length]\n",
    "\n",
    "    texts = batch['content'].tolist()\n",
    "    ner_results = []\n",
    "\n",
    "    for doc, row in zip(nlp.pipe(texts, batch_size=50), batch.itertuples()):\n",
    "        try:\n",
    "            ents = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping post {row.post_id} due to error: {e}\")\n",
    "            skipped_posts.append(row.post_id)\n",
    "            ents = []\n",
    "\n",
    "        ner_results.append(ents)\n",
    "\n",
    "    batch['entities'] = ner_results\n",
    "\n",
    "    # Construct the full file path\n",
    "    file_name = f'ner_results_batch_{year}.{month}_{start}{special_identifier}.csv'\n",
    "    full_file_path = os.path.join(output_directory, file_name)\n",
    "\n",
    "    # Save the DataFrame to the CSV file in the new directory\n",
    "    batch.to_csv(full_file_path, index=False)\n",
    "\n",
    "# Save skipped post IDs\n",
    "pd.Series(skipped_posts).to_csv('skipped_posts.csv', index=False)\n",
    "\n",
    "print(\"NER complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "files = glob.glob(os.path.join(output_directory, f\"ner_results_batch_*{special_identifier}.csv\"))\n",
    "dfs = [pd.read_csv(os.path.join(f)) for f in files]\n",
    "\n",
    "final_ner_df = pd.concat(dfs, ignore_index=True)\n",
    "final_ner_df.to_csv(os.path.join(output_directory, f'ner_results_append_sum_{year}.{month}{special_identifier}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import ast\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "entity_counter = Counter()\n",
    "\n",
    "for entities in tqdm(final_ner_df['entities'], desc=\"Processing Entities\"):\n",
    "    try:\n",
    "        ents = ast.literal_eval(entities)\n",
    "        entity_counter.update([e[0] for e in ents])\n",
    "    except (SyntaxError, ValueError) as e:\n",
    "        print(f\"Error parsing entities: {e} for input: {entities}\")\n",
    "        continue  # Skip to the next item if there's an error\n",
    "\n",
    "print(entity_counter.most_common(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit_counter = final_ner_df['subreddit'].value_counts()\n",
    "\n",
    "print(subreddit_counter.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "def get_sentiment(text):\n",
    "    if isinstance(text, str):\n",
    "        return analyzer.polarity_scores(text)['compound']\n",
    "    return 0.0  # Neutral if empty or NaN\n",
    "\n",
    "# final_ner_df['sentiment'] = final_ner_df['content'].apply(get_sentiment)\n",
    "final_ner_df['sentiment'] = final_ner_df['content'].progress_apply(get_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_ner_df.to_csv(os.path.join(output_directory, f'ner_results_append_sum_{year}_{month}_sentiment{special_identifier}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the plot output directory\n",
    "plot_output_directory = f\"batch{special_identifier}/PLOTS\"\n",
    "\n",
    "# Create the PLOTS directory if it doesn't exist\n",
    "os.makedirs(plot_output_directory, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract SERVICE entities and their sentiment and timestamp\n",
    "service_post_counts = []\n",
    "\n",
    "for row in final_ner_df.itertuples():\n",
    "    try:\n",
    "        ents = ast.literal_eval(row.entities)\n",
    "        # Filter for 'SERVICE' entities that are in your predefined list\n",
    "        services_in_post = [e[0] for e in ents if e[1] == 'SERVICE' and e[0] in services]\n",
    "        for service in services_in_post:\n",
    "            service_post_counts.append((service, row.timestamp)) # Include timestamp\n",
    "    except (SyntaxError, ValueError) as e:\n",
    "        print(f\"Error parsing entities: {e} for row with index {row.Index}\")\n",
    "        continue\n",
    "\n",
    "# Create a DataFrame to store service and timestamp\n",
    "service_timestamp_df = pd.DataFrame(service_post_counts, columns=['service', 'timestamp'])\n",
    "\n",
    "# Convert timestamp to a datetime object if it's not already\n",
    "if not pd.api.types.is_datetime64_any_dtype(service_timestamp_df['timestamp']):\n",
    "    service_timestamp_df['timestamp'] = pd.to_datetime(service_timestamp_df['timestamp'], unit='s') # Assuming timestamp is in seconds since epoch, adjust 'unit' if needed\n",
    "\n",
    "# Extract the date part from the timestamp\n",
    "service_timestamp_df['date'] = service_timestamp_df['timestamp'].dt.date\n",
    "\n",
    "# Group by service and date and count the number of posts\n",
    "post_counts_per_day = service_timestamp_df.groupby(['service', 'date']).size().reset_index(name='post_count')\n",
    "\n",
    "# --- Plotting ---\n",
    "plt.figure(figsize=(12, 6))  # Adjust figure size as needed\n",
    "\n",
    "for service in services:\n",
    "    service_data = post_counts_per_day[post_counts_per_day['service'] == service]\n",
    "    plt.plot(service_data['date'], service_data['post_count'], label=service)\n",
    "\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Post Count\")\n",
    "plt.title(\"Frequency of Posts per Service per Day\")\n",
    "plt.legend(title=\"Entity\")\n",
    "plt.grid(True)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# Save the plot to the specified directory\n",
    "plot_filename = os.path.join(plot_output_directory, \"post_frequency_per_service.png\")\n",
    "plt.savefig(plot_filename)\n",
    "plt.close() # Close the plot to prevent display in notebook if running in batch\n",
    "\n",
    "print(f\"Plot saved to: {plot_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle\n",
    "\n",
    "# Extract SERVICE entities and their sentiment and timestamp\n",
    "service_post_counts = []\n",
    "\n",
    "for row in final_ner_df.itertuples():\n",
    "    try:\n",
    "        ents = ast.literal_eval(row.entities)\n",
    "        # Filter for 'SERVICE' entities that are in your predefined list\n",
    "        services_in_post = [e[0] for e in ents if e[1] == 'SERVICE' and e[0] in services]\n",
    "        for service in services_in_post:\n",
    "            service_post_counts.append((service, row.timestamp)) # Include timestamp\n",
    "    except (SyntaxError, ValueError) as e:\n",
    "        print(f\"Error parsing entities: {e} for row with index {row.Index}\")\n",
    "        continue\n",
    "\n",
    "# Create a DataFrame to store service and timestamp\n",
    "service_timestamp_df = pd.DataFrame(service_post_counts, columns=['service', 'timestamp'])\n",
    "\n",
    "# Convert timestamp to a datetime object if it's not already\n",
    "if not pd.api.types.is_datetime64_any_dtype(service_timestamp_df['timestamp']):\n",
    "    service_timestamp_df['timestamp'] = pd.to_datetime(service_timestamp_df['timestamp'], unit='s') # Assuming timestamp is in seconds since epoch, adjust 'unit' if needed\n",
    "\n",
    "# Extract the date part from the timestamp\n",
    "service_timestamp_df['date'] = service_timestamp_df['timestamp'].dt.date\n",
    "\n",
    "# Group by service and date and count the number of posts\n",
    "post_counts_per_day = service_timestamp_df.groupby(['service', 'date']).size().reset_index(name='post_count')\n",
    "\n",
    "# --- Overlapping Bar Chart Plotting ---\n",
    "plt.figure(figsize=(14, 7))  # Adjust figure size as needed\n",
    "\n",
    "colors = cycle(plt.cm.tab10.colors) # Cycle through a set of distinct colors\n",
    "opacity = 0.6\n",
    "\n",
    "for service in services:\n",
    "    service_data = post_counts_per_day[post_counts_per_day['service'] == service]\n",
    "    color = next(colors)\n",
    "    plt.bar(service_data['date'], service_data['post_count'], label=service, color=color, alpha=opacity)\n",
    "\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Post Count\")\n",
    "plt.title(\"Frequency of Posts per Service per Day (Overlapping Bar Charts)\")\n",
    "plt.legend(title=\"Entity\")\n",
    "plt.grid(axis='y', linestyle='--')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# Save the plot to the specified directory\n",
    "plot_filename = os.path.join(plot_output_directory, \"post_frequency_per_service_barchart.png\")\n",
    "plt.savefig(plot_filename)\n",
    "plt.close() # Close the plot to prevent display in notebook if running in batch\n",
    "\n",
    "print(f\"Plot saved to: {plot_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract SERVICE entities and their sentiment and timestamp\n",
    "service_post_counts_all = []\n",
    "\n",
    "for row in final_ner_df.itertuples():\n",
    "    try:\n",
    "        ents = ast.literal_eval(row.entities)\n",
    "        services_in_post = [e[0] for e in ents if e[1] == 'SERVICE' and e[0] in services]\n",
    "        for service in services_in_post:\n",
    "            service_post_counts_all.append((service, row.timestamp))\n",
    "    except (SyntaxError, ValueError) as e:\n",
    "        print(f\"Error parsing entities: {e} for row with index {row.Index}\")\n",
    "        continue\n",
    "\n",
    "service_timestamp_df_all = pd.DataFrame(service_post_counts_all, columns=['service', 'timestamp'])\n",
    "\n",
    "if not pd.api.types.is_datetime64_any_dtype(service_timestamp_df_all['timestamp']):\n",
    "    service_timestamp_df_all['timestamp'] = pd.to_datetime(service_timestamp_df_all['timestamp'], unit='s')\n",
    "\n",
    "service_timestamp_df_all['date'] = service_timestamp_df_all['timestamp'].dt.date\n",
    "\n",
    "post_counts_per_day_all = service_timestamp_df_all.groupby(['service', 'date']).size().reset_index(name='post_count')\n",
    "\n",
    "# Calculate total post counts per service\n",
    "total_posts_per_service = post_counts_per_day_all.groupby('service')['post_count'].sum().sort_values(ascending=False)\n",
    "\n",
    "# Divide services into three groups\n",
    "n_services = len(services)\n",
    "third = n_services // 3\n",
    "top_third_services = total_posts_per_service.index[:third]\n",
    "middle_third_services = total_posts_per_service.index[third:2*third]\n",
    "bottom_third_services = total_posts_per_service.index[2*third:]\n",
    "\n",
    "# --- Plotting ---\n",
    "fig, axes = plt.subplots(3, 1, figsize=(14, 12), sharex=True)\n",
    "fig.suptitle(\"Frequency of Posts per Service per Day\", fontsize=16, y=1.02)\n",
    "\n",
    "def plot_service_group(ax, services_to_plot, title):\n",
    "    for service in services_to_plot:\n",
    "        service_data = post_counts_per_day_all[post_counts_per_day_all['service'] == service]\n",
    "        ax.bar(service_data['date'], service_data['post_count'], label=service, alpha=0.7)\n",
    "    ax.set_ylabel(\"Post Count\")\n",
    "    ax.set_title(title)\n",
    "    ax.tick_params(axis='x', rotation=45, labelbottom=True)\n",
    "    ax.legend(title=\"Entity\")\n",
    "    ax.grid(axis='y', linestyle='--')\n",
    "\n",
    "plot_service_group(axes[0], top_third_services, f\"Top {len(top_third_services)} Most Frequent Services\")\n",
    "plot_service_group(axes[1], middle_third_services, f\"Next {len(middle_third_services)} Most Frequent Services\")\n",
    "plot_service_group(axes[2], bottom_third_services, f\"Remaining {len(bottom_third_services)} Services\")\n",
    "\n",
    "fig.text(0.5, 0.01, 'Date', ha='center')\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "# plt.show()\n",
    "\n",
    "# Save the plot to the specified directory\n",
    "plot_filename = os.path.join(plot_output_directory, \"post_frequency_per_service_volatility.png\")\n",
    "plt.savefig(plot_filename)\n",
    "plt.close() # Close the plot to prevent display in notebook if running in batch\n",
    "\n",
    "print(f\"Plot saved to: {plot_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle\n",
    "\n",
    "# Extract SERVICE entities and their sentiment and timestamp\n",
    "entity_sentiments_over_time = []\n",
    "\n",
    "for row in final_ner_df.itertuples():\n",
    "    try:\n",
    "        ents = ast.literal_eval(row.entities)\n",
    "        date = pd.to_datetime(row.timestamp, unit='s').date() # Extract date\n",
    "        for ent_text, ent_label in ents:\n",
    "            if ent_label == 'SERVICE' and ent_text in services:\n",
    "                entity_sentiments_over_time.append((ent_text, date, row.sentiment))\n",
    "    except (SyntaxError, ValueError) as e:\n",
    "        print(f\"Error parsing entities: {e} for row with index {row.Index}\")\n",
    "        continue\n",
    "\n",
    "# Create a DataFrame for entity sentiment over time\n",
    "entity_sentiment_df = pd.DataFrame(entity_sentiments_over_time, columns=['service', 'date', 'sentiment'])\n",
    "\n",
    "# Calculate average sentiment per entity per day\n",
    "avg_sentiment_per_day = entity_sentiment_df.groupby(['service', 'date'])['sentiment'].mean().reset_index()\n",
    "\n",
    "# --- Plotting Sentiment vs. Date ---\n",
    "plt.figure(figsize=(14, 7))\n",
    "colors = cycle(plt.cm.tab10.colors)\n",
    "\n",
    "for service in services:\n",
    "    service_data = avg_sentiment_per_day[avg_sentiment_per_day['service'] == service]\n",
    "    color = next(colors)\n",
    "    plt.plot(service_data['date'], service_data['sentiment'], label=service, color=color)\n",
    "\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Average Sentiment\")\n",
    "plt.title(\"Sentiment of Posts Over Time by Entity\")\n",
    "plt.legend(title=\"Entity\") # Changed legend title here\n",
    "plt.grid(True)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# Save the plot to the specified directory\n",
    "plot_filename = os.path.join(plot_output_directory, \"post_sentiment_by_entity.png\")\n",
    "plt.savefig(plot_filename)\n",
    "plt.close() # Close the plot to prevent display in notebook if running in batch\n",
    "\n",
    "print(f\"Plot saved to: {plot_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Extract SERVICE entities and their sentiment and timestamp\n",
    "entity_sentiments_over_time = []\n",
    "\n",
    "for row in tqdm(final_ner_df.itertuples(), total=len(final_ner_df), desc=\"Extracting Entity Sentiments\"):\n",
    "    try:\n",
    "        ents = ast.literal_eval(row.entities)\n",
    "        date = pd.to_datetime(row.timestamp, unit='s').date() # Extract date\n",
    "        for ent_text, ent_label in ents:\n",
    "            if ent_label == 'SERVICE' and ent_text in services:\n",
    "                entity_sentiments_over_time.append((ent_text, date, row.sentiment))\n",
    "    except (SyntaxError, ValueError) as e:\n",
    "        print(f\"Error parsing entities: {e} for row with index {row.Index}\")\n",
    "        continue\n",
    "\n",
    "# Create a DataFrame for entity sentiment over time\n",
    "entity_sentiment_df = pd.DataFrame(entity_sentiments_over_time, columns=['service', 'date', 'sentiment'])\n",
    "\n",
    "# Calculate average sentiment per entity per day\n",
    "avg_sentiment_per_day = entity_sentiment_df.groupby(['service', 'date'])['sentiment'].mean().reset_index()\n",
    "\n",
    "# Calculate volatility (standard deviation of sentiment) per service\n",
    "sentiment_volatility = avg_sentiment_per_day.groupby('service')['sentiment'].std().sort_values(ascending=False)\n",
    "\n",
    "# Divide services into three groups based on volatility\n",
    "n_services = len(services)\n",
    "third = n_services // 3\n",
    "high_volatility_services = sentiment_volatility.index[:third].tolist()\n",
    "medium_volatility_services = sentiment_volatility.index[third:2*third].tolist()\n",
    "low_volatility_services = sentiment_volatility.index[2*third:].tolist()\n",
    "\n",
    "# --- Plotting Sentiment vs. Date by Volatility ---\n",
    "fig, axes = plt.subplots(3, 1, figsize=(16, 12), sharex=True)\n",
    "fig.suptitle(\"Average Sentiment of Posts Over Time by Entity Volatility\", fontsize=16, y=1.02)\n",
    "\n",
    "def plot_sentiment_group(ax, services_to_plot, title):\n",
    "    colors = cycle(plt.cm.tab10.colors)\n",
    "    for service in services_to_plot:\n",
    "        service_data = avg_sentiment_per_day[avg_sentiment_per_day['service'] == service]\n",
    "        color = next(colors)\n",
    "        ax.plot(service_data['date'], service_data['sentiment'], label=service, color=color)\n",
    "    ax.set_ylabel(\"Average Sentiment\")\n",
    "    ax.set_title(title)\n",
    "    ax.tick_params(axis='x', rotation=45, labelbottom=True)\n",
    "    ax.legend(title=\"Entity\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    ax.grid(True)\n",
    "\n",
    "plot_sentiment_group(axes[0], high_volatility_services, f\"High Volatility Entities\")\n",
    "plot_sentiment_group(axes[1], medium_volatility_services, f\"Medium Volatility Entities\")\n",
    "plot_sentiment_group(axes[2], low_volatility_services, f\"Low Volatility Entities\")\n",
    "\n",
    "fig.text(0.5, 0.01, 'Date', ha='center')\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "# plt.show()\n",
    "\n",
    "# Save the plot to the specified directory\n",
    "plot_filename = os.path.join(plot_output_directory, \"post_sentiment_by_entity_by_volatility.png\")\n",
    "plt.savefig(plot_filename)\n",
    "plt.close() # Close the plot to prevent display in notebook if running in batch\n",
    "\n",
    "print(f\"Plot saved to: {plot_filename}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
