{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gabri\\anaconda3\\envs\\data_mining\\lib\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import reddit_credentials as rc\n",
    "\n",
    "# Initialize Reddit API (replace with your credentials)\n",
    "reddit = praw.Reddit(client_id=rc.client_id,\n",
    "                     client_secret=rc.client_secret,\n",
    "                     user_agent=rc.user_agent)\n",
    "\n",
    "# Subreddits to analyze\n",
    "subreddits = [\"moving\", \"florida\", \"realestate\", \"jobs\", \"tax\"]\n",
    "\n",
    "posts = []\n",
    "for sub in subreddits:\n",
    "    for submission in reddit.subreddit(sub).search(\"moving to Florida\", limit=1000):\n",
    "        posts.append([submission.created_utc, submission.title, submission.selftext, sub])\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(posts, columns=[\"created_utc\", \"title\", \"selftext\", \"subreddit\"])\n",
    "df[\"created_utc\"] = pd.to_datetime(df[\"created_utc\"], unit=\"s\")\n",
    "\n",
    "# Merge title and selftext for classification\n",
    "df[\"text\"] = df[\"title\"] + \" \" + df[\"selftext\"]\n",
    "df = df[[\"created_utc\", \"text\", \"subreddit\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 0.88\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[\"text\"], df[\"subreddit\"], test_size=0.2, random_state=42)\n",
    "\n",
    "# Build pipeline\n",
    "pipeline = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer(stop_words=\"english\", max_features=5000)),\n",
    "    (\"clf\", LogisticRegression())\n",
    "])\n",
    "\n",
    "# Train model\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate model\n",
    "y_pred = pipeline.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Model Accuracy: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65cc4f39209f4fbd9780e6a308fa66a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12e1a7505b0d442fa9f335ab5cde8ce8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6c03742df2e44f7be9d70be7ba84c0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54fce8c6a3ed43bfabde644e92146cf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a490d1661a4142ebacd74d7b4feee8c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71f653459de04d4f838d628fcdeccbae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a594f4df48f4f008d08873090085937",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebb013959fa34b859c3ea6bdb2018cea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57827cf8fdd944b8a10ece3338eeb289",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90b41f853c4b4a83a7b74d946f6518ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c425fa5b755341b8a985d502b11d78be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from bertopic import BERTopic\n",
    "\n",
    "# Train topic model\n",
    "topic_model = BERTopic()\n",
    "topics, _ = topic_model.fit_transform(df[\"text\"])\n",
    "\n",
    "# Add topics to DataFrame\n",
    "df[\"topic\"] = topics\n",
    "\n",
    "# Aggregate topics over time\n",
    "df[\"month\"] = df[\"created_utc\"].dt.to_period(\"M\").astype(str)\n",
    "df_topic_trends = df.groupby([\"month\", \"topic\"]).size().reset_index(name=\"post_count\")\n",
    "\n",
    "# Convert to wide format for forecasting\n",
    "df_pivot = df_topic_trends.pivot(index=\"month\", columns=\"topic\", values=\"post_count\").fillna(0)\n",
    "df_pivot.index = pd.to_datetime(df_pivot.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18:55:07 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:55:08 - cmdstanpy - INFO - Chain [1] done processing\n",
      "18:55:08 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:55:08 - cmdstanpy - INFO - Chain [1] done processing\n",
      "18:55:08 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:55:08 - cmdstanpy - INFO - Chain [1] done processing\n",
      "18:55:09 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:55:09 - cmdstanpy - INFO - Chain [1] done processing\n",
      "18:55:09 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:55:09 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    }
   ],
   "source": [
    "from prophet import Prophet\n",
    "\n",
    "# Forecast each topic\n",
    "topic_forecasts = {}\n",
    "\n",
    "for topic in df_pivot.columns:\n",
    "    df_topic = df_pivot[[topic]].reset_index()\n",
    "    df_topic.rename(columns={\"month\": \"ds\", topic: \"y\"}, inplace=True)\n",
    "\n",
    "    # Train Prophet model\n",
    "    model = Prophet()\n",
    "    model.fit(df_topic)\n",
    "\n",
    "    # Predict future topics\n",
    "    future = model.make_future_dataframe(periods=12, freq=\"M\")\n",
    "    forecast = model.predict(future)\n",
    "\n",
    "    # Store predictions\n",
    "    topic_forecasts[topic] = forecast[[\"ds\", \"yhat\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted subreddit: florida\n"
     ]
    }
   ],
   "source": [
    "# Example: Generate synthetic future discussion topics\n",
    "future_topics = {topic: topic_forecasts[topic][\"yhat\"].values[-1] for topic in topic_forecasts}\n",
    "\n",
    "# Simulate a new post (e.g., rising topic)\n",
    "topic_id = list(future_topics.keys())[0]\n",
    "sample_text = \" \".join([word for word, _ in topic_model.get_topic(topic_id)])\n",
    "\n",
    "# Predict the subreddit\n",
    "predicted_subreddit = pipeline.predict([sample_text])[0]\n",
    "print(f\"Predicted subreddit: {predicted_subreddit}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_mining",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
