{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import networkx as nx\n",
    "from collections import Counter\n",
    "import creds\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uncomment these to have full output on jupyter\n",
    "#pd.set_option('display.max_colwidth', None)\n",
    "#pd.set_option('display.max_rows', None)\n",
    "#pd.set_option('display.width', None)  # This one is important for terminal-like output\n",
    "#pd.set_option('display.expand_frame_repr', False) # Prevents line wrapping.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.reddit.com/r/pushshift/comments/1itme1k/separate_dump_files_for_the_top_40k_subreddits/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "# !{sys.executable} -m pip install spacy\n",
    "# !{sys.executable} -m spacy download en_core_web_sm\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Main function to orchestrate the process.\"\"\"\n",
    "reddit = praw.Reddit(\n",
    "    client_id=creds.CLIENT_ID,\n",
    "    client_secret=creds.CLIENT_SECRET,\n",
    "    user_agent=creds.USER_AGENTS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# def fetch_reddit_posts(keywords, subreddits, days, limit):\n",
    "#     end_time = datetime.datetime.now(datetime.UTC)\n",
    "#     start_time = end_time - datetime.timedelta(days=days)\n",
    "\n",
    "#     posts = []\n",
    "#     for subreddit in subreddits:\n",
    "#         current_end_time = end_time.timestamp()\n",
    "#         while current_end_time > start_time.timestamp() and len(posts) < limit:\n",
    "#             print(f\"Fetching posts before: {datetime.datetime.fromtimestamp(current_end_time, datetime.UTC)}\")\n",
    "#             for submission in reddit.subreddit(subreddit).search(\n",
    "#                 \" OR \".join(keywords),\n",
    "#                 sort=\"new\",\n",
    "#                 limit=min(100, limit - len(posts)),  # Fetch up to 100 at a time, respecting the overall limit\n",
    "#                 params={'before': int(current_end_time)}  # Pass 'before' in the params dictionary\n",
    "#             ):\n",
    "#                 if submission.created_utc < start_time.timestamp():\n",
    "#                     break  # Stop if we've gone past the start time\n",
    "\n",
    "#                 posts.append({\n",
    "#                     \"subreddit\": subreddit,\n",
    "#                     \"title\": submission.title,\n",
    "#                     \"text\": submission.selftext,\n",
    "#                     \"created_utc\": submission.created_utc\n",
    "#                 })\n",
    "#             if posts:\n",
    "#                 current_end_time = posts[-1]['created_utc']\n",
    "#             else:\n",
    "#                 break # No more posts found for this time range\n",
    "\n",
    "#     return pd.DataFrame(posts)\n",
    "\n",
    "# keywords = [\"lazy\", \"lousy\", \"dirty\", \"bad\", \"terrible\", \"horrible\", \"unreliable\"]\n",
    "# subreddits = [\"all\"]\n",
    "# #subreddits = [\"airlines\", \"Comcast\", \"healthcare\", \"railways\"]\n",
    "# df = fetch_reddit_posts(keywords, subreddits, days=1000, limit=10000)\n",
    "# df[\"created_date\"] = pd.to_datetime(df[\"created_utc\"], unit=\"s\")\n",
    "# print(\"DF Head---------------------------------\")\n",
    "# print(df.head())\n",
    "# print(\"DF Tail---------------------------------\")\n",
    "# print(df.tail())\n",
    "# print(f\"Total posts fetched: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End Time of Interval being checked: 2025-03-31 22:05:31.773405+00:00\n",
      "Adding Post - ID: 1jofkte, UTC: 1743458709.0, Title: What are more ways to make this kink fun?, Text: I've been participating in fin...\n",
      "End Time of Interval being checked: 2025-03-31 21:05:31.773405+00:00\n",
      "End Time of Interval being checked: 2025-03-31 20:05:31.773405+00:00\n",
      "End Time of Interval being checked: 2025-03-31 19:05:31.773405+00:00\n",
      "End Time of Interval being checked: 2025-03-31 18:05:31.773405+00:00\n",
      "End Time of Interval being checked: 2025-03-31 17:05:31.773405+00:00\n",
      "End Time of Interval being checked: 2025-03-31 16:05:31.773405+00:00\n",
      "End Time of Interval being checked: 2025-03-31 15:05:31.773405+00:00\n",
      "End Time of Interval being checked: 2025-03-31 14:05:31.773405+00:00\n",
      "End Time of Interval being checked: 2025-03-31 13:05:31.773405+00:00\n",
      "End Time of Interval being checked: 2025-03-31 12:05:31.773405+00:00\n",
      "End Time of Interval being checked: 2025-03-31 11:05:31.773405+00:00\n",
      "End Time of Interval being checked: 2025-03-31 10:05:31.773405+00:00\n",
      "End Time of Interval being checked: 2025-03-31 09:05:31.773405+00:00\n",
      "End Time of Interval being checked: 2025-03-31 08:05:31.773405+00:00\n",
      "End Time of Interval being checked: 2025-03-31 07:05:31.773405+00:00\n",
      "End Time of Interval being checked: 2025-03-31 06:05:31.773405+00:00\n",
      "End Time of Interval being checked: 2025-03-31 05:05:31.773405+00:00\n",
      "End Time of Interval being checked: 2025-03-31 04:05:31.773405+00:00\n",
      "End Time of Interval being checked: 2025-03-31 03:05:31.773405+00:00\n",
      "End Time of Interval being checked: 2025-03-31 02:05:31.773405+00:00\n",
      "End Time of Interval being checked: 2025-03-31 01:05:31.773405+00:00\n",
      "End Time of Interval being checked: 2025-03-31 00:05:31.773405+00:00\n",
      "End Time of Interval being checked: 2025-03-30 23:05:31.773405+00:00\n",
      "End Time of Interval being checked: 2025-03-30 22:05:31.773405+00:00\n",
      "End Time of Interval being checked: 2025-03-30 21:05:31.773405+00:00\n",
      "End Time of Interval being checked: 2025-03-30 20:05:31.773405+00:00\n",
      "End Time of Interval being checked: 2025-03-30 19:05:31.773405+00:00\n",
      "End Time of Interval being checked: 2025-03-30 18:05:31.773405+00:00\n",
      "End Time of Interval being checked: 2025-03-30 17:05:31.773405+00:00\n",
      "End Time of Interval being checked: 2025-03-30 16:05:31.773405+00:00\n",
      "End Time of Interval being checked: 2025-03-30 15:05:31.773405+00:00\n",
      "End Time of Interval being checked: 2025-03-30 14:05:31.773405+00:00\n",
      "End Time of Interval being checked: 2025-03-30 13:05:31.773405+00:00\n",
      "End Time of Interval being checked: 2025-03-30 12:05:31.773405+00:00\n",
      "End Time of Interval being checked: 2025-03-30 11:05:31.773405+00:00\n",
      "End Time of Interval being checked: 2025-03-30 10:05:31.773405+00:00\n",
      "End Time of Interval being checked: 2025-03-30 09:05:31.773405+00:00\n",
      "End Time of Interval being checked: 2025-03-30 08:05:31.773405+00:00\n",
      "End Time of Interval being checked: 2025-03-30 07:05:31.773405+00:00\n",
      "End Time of Interval being checked: 2025-03-30 06:05:31.773405+00:00\n",
      "End Time of Interval being checked: 2025-03-30 05:05:31.773405+00:00\n",
      "End Time of Interval being checked: 2025-03-30 04:05:31.773405+00:00\n",
      "End Time of Interval being checked: 2025-03-30 03:05:31.773405+00:00\n",
      "End Time of Interval being checked: 2025-03-30 02:05:31.773405+00:00\n",
      "End Time of Interval being checked: 2025-03-30 01:05:31.773405+00:00\n",
      "End Time of Interval being checked: 2025-03-30 00:05:31.773405+00:00\n",
      "End Time of Interval being checked: 2025-03-29 23:05:31.773405+00:00\n",
      "End Time of Interval being checked: 2025-03-29 22:05:31.773405+00:00\n",
      "End Time of Interval being checked: 2025-03-29 21:05:31.773405+00:00\n",
      "End Time of Interval being checked: 2025-03-29 20:05:31.773405+00:00\n",
      "End Time of Interval being checked: 2025-03-29 19:05:31.773405+00:00\n",
      "End Time of Interval being checked: 2025-03-29 18:05:31.773405+00:00\n",
      "End Time of Interval being checked: 2025-03-29 17:05:31.773405+00:00\n",
      "End Time of Interval being checked: 2025-03-29 16:05:31.773405+00:00\n",
      "End Time of Interval being checked: 2025-03-29 15:05:31.773405+00:00\n",
      "End Time of Interval being checked: 2025-03-29 14:05:31.773405+00:00\n",
      "End Time of Interval being checked: 2025-03-29 13:05:31.773405+00:00\n",
      "End Time of Interval being checked: 2025-03-29 12:05:31.773405+00:00\n",
      "End Time of Interval being checked: 2025-03-29 11:05:31.773405+00:00\n",
      "End Time of Interval being checked: 2025-03-29 10:05:31.773405+00:00\n",
      "End Time of Interval being checked: 2025-03-29 09:05:31.773405+00:00\n",
      "End Time of Interval being checked: 2025-03-29 08:05:31.773405+00:00\n",
      "End Time of Interval being checked: 2025-03-29 07:05:31.773405+00:00\n",
      "End Time of Interval being checked: 2025-03-29 06:05:31.773405+00:00\n",
      "End Time of Interval being checked: 2025-03-29 05:05:31.773405+00:00\n",
      "End Time of Interval being checked: 2025-03-29 04:05:31.773405+00:00\n",
      "End Time of Interval being checked: 2025-03-29 03:05:31.773405+00:00\n",
      "End Time of Interval being checked: 2025-03-29 02:05:31.773405+00:00\n",
      "End Time of Interval being checked: 2025-03-29 01:05:31.773405+00:00\n",
      "End Time of Interval being checked: 2025-03-29 00:05:31.773405+00:00\n",
      "End Time of Interval being checked: 2025-03-28 23:05:31.773405+00:00\n",
      "End Time of Interval being checked: 2025-03-28 22:05:31.773405+00:00\n",
      "End Time of Interval being checked: 2025-03-28 21:05:31.773405+00:00\n",
      "End Time of Interval being checked: 2025-03-28 20:05:31.773405+00:00\n",
      "End Time of Interval being checked: 2025-03-28 19:05:31.773405+00:00\n",
      "End Time of Interval being checked: 2025-03-28 18:05:31.773405+00:00\n",
      "End Time of Interval being checked: 2025-03-28 17:05:31.773405+00:00\n",
      "End Time of Interval being checked: 2025-03-28 16:05:31.773405+00:00\n",
      "End Time of Interval being checked: 2025-03-28 15:05:31.773405+00:00\n",
      "End Time of Interval being checked: 2025-03-28 14:05:31.773405+00:00\n",
      "End Time of Interval being checked: 2025-03-28 13:05:31.773405+00:00\n",
      "End Time of Interval being checked: 2025-03-28 12:05:31.773405+00:00\n",
      "End Time of Interval being checked: 2025-03-28 11:05:31.773405+00:00\n",
      "End Time of Interval being checked: 2025-03-28 10:05:31.773405+00:00\n",
      "End Time of Interval being checked: 2025-03-28 09:05:31.773405+00:00\n",
      "End Time of Interval being checked: 2025-03-28 08:05:31.773405+00:00\n",
      "End Time of Interval being checked: 2025-03-28 07:05:31.773405+00:00\n",
      "End Time of Interval being checked: 2025-03-28 06:05:31.773405+00:00\n",
      "End Time of Interval being checked: 2025-03-28 05:05:31.773405+00:00\n",
      "End Time of Interval being checked: 2025-03-28 04:05:31.773405+00:00\n",
      "End Time of Interval being checked: 2025-03-28 03:05:31.773405+00:00\n",
      "End Time of Interval being checked: 2025-03-28 02:05:31.773405+00:00\n",
      "End Time of Interval being checked: 2025-03-28 01:05:31.773405+00:00\n",
      "End Time of Interval being checked: 2025-03-28 00:05:31.773405+00:00\n",
      "End Time of Interval being checked: 2025-03-27 23:05:31.773405+00:00\n",
      "End Time of Interval being checked: 2025-03-27 22:05:31.773405+00:00\n",
      "End Time of Interval being checked: 2025-03-27 21:05:31.773405+00:00\n",
      "End Time of Interval being checked: 2025-03-27 20:05:31.773405+00:00\n",
      "End Time of Interval being checked: 2025-03-27 19:05:31.773405+00:00\n",
      "End Time of Interval being checked: 2025-03-27 18:05:31.773405+00:00\n",
      "End Time of Interval being checked: 2025-03-27 17:05:31.773405+00:00\n",
      "End Time of Interval being checked: 2025-03-27 16:05:31.773405+00:00\n",
      "End Time of Interval being checked: 2025-03-27 15:05:31.773405+00:00\n",
      "End Time of Interval being checked: 2025-03-27 14:05:31.773405+00:00\n",
      "End Time of Interval being checked: 2025-03-27 13:05:31.773405+00:00\n",
      "End Time of Interval being checked: 2025-03-27 12:05:31.773405+00:00\n",
      "End Time of Interval being checked: 2025-03-27 11:05:31.773405+00:00\n",
      "End Time of Interval being checked: 2025-03-27 10:05:31.773405+00:00\n",
      "End Time of Interval being checked: 2025-03-27 09:05:31.773405+00:00\n",
      "End Time of Interval being checked: 2025-03-27 08:05:31.773405+00:00\n",
      "End Time of Interval being checked: 2025-03-27 07:05:31.773405+00:00\n",
      "End Time of Interval being checked: 2025-03-27 06:05:31.773405+00:00\n",
      "End Time of Interval being checked: 2025-03-27 05:05:31.773405+00:00\n",
      "End Time of Interval being checked: 2025-03-27 04:05:31.773405+00:00\n"
     ]
    }
   ],
   "source": [
    "#Same as above but limits post collection to one every X minute timespan\n",
    "def fetch_reddit_posts_fast_intervals(keywords, subreddits, days, limit, interval_minutes=10):\n",
    "    \"\"\"\n",
    "    Fetches Reddit posts, finding the first matching post in each interval and skipping to the next.\n",
    "\n",
    "    Args:\n",
    "        keywords (list): List of keywords to search for.\n",
    "        subreddits (list): List of subreddits to search in.\n",
    "        days (int): Number of days to search back.\n",
    "        limit (int): Maximum number of posts to retrieve.\n",
    "        interval_minutes (int, optional): Time interval in minutes. Defaults to 10.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing fetched posts.\n",
    "    \"\"\"\n",
    "\n",
    "    end_time = datetime.datetime.now(datetime.UTC)\n",
    "    start_time = end_time - datetime.timedelta(days=days)\n",
    "    interval_seconds = interval_minutes * 60\n",
    "\n",
    "    posts = []\n",
    "    current_time = end_time\n",
    "    added_post_ids = set()  # Track post IDs to prevent duplicates\n",
    "\n",
    "    while current_time > start_time and len(posts) < limit:\n",
    "        interval_start = current_time - datetime.timedelta(minutes=interval_minutes)\n",
    "        interval_end = current_time\n",
    "        print(f\"End Time of Interval being checked: {interval_end}\")\n",
    "        \n",
    "        found_post = False\n",
    "        for subreddit in subreddits:\n",
    "            if found_post:\n",
    "                break #if we found a post, move on to the next subreddit.\n",
    "            for submission in reddit.subreddit(subreddit).search(\n",
    "                \" OR \".join(keywords),\n",
    "                sort=\"new\",\n",
    "                time_filter=\"all\",\n",
    "                params={'before': int(interval_end.timestamp()), 'after': int(interval_start.timestamp())}\n",
    "            ):\n",
    "                if submission.id not in added_post_ids: #check if the post was already added.\n",
    "                    #Print something so we see it running\n",
    "                    post_text = (submission.selftext or \"\")  # Handle cases where selftext is None\n",
    "                    post_text_preview = post_text[:30]\n",
    "                    print(f\"Adding Post - ID: {submission.id}, UTC: {submission.created_utc}, Title: {submission.title}, Text: {post_text_preview}...\")\n",
    "\n",
    "                    posts.append({\n",
    "                        \"subreddit\": submission.subreddit.display_name,\n",
    "                        \"title\": submission.title,\n",
    "                        \"text\": submission.selftext,\n",
    "                        \"created_utc\": submission.created_utc\n",
    "                    })\n",
    "                added_post_ids.add(submission.id) #add post id to set.\n",
    "                found_post = True\n",
    "                break #we found a post, move on to the next subreddit.\n",
    "\n",
    "        current_time = interval_start\n",
    "\n",
    "    return pd.DataFrame(posts)\n",
    "\n",
    "keywords = [\"lazy\", \"lousy\", \"dirty\", \"bad\", \"terrible\", \"horrible\", \"unreliable\", \"wrong\", \"hate\", \"sucks\"]\n",
    "subreddits = [\"all\"]\n",
    "df = fetch_reddit_posts_fast_intervals(keywords, subreddits, days=10000, limit=10000, interval_minutes=60)\n",
    "df[\"created_date\"] = pd.to_datetime(df[\"created_utc\"], unit=\"s\")\n",
    "print(\"DF Head---------------------------------\")\n",
    "print(df.head())\n",
    "print(\"DF Tail---------------------------------\")\n",
    "print(df.tail())\n",
    "print(f\"Total posts fetched: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns\n",
    "df.to_csv(\"df.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# def extract_entities(text):\n",
    "#     doc = nlp(text)\n",
    "#     entities = [ent.text for ent in doc.ents if ent.label_ in [\"ORG\", \"GPE\", \"PRODUCT\"]]\n",
    "#     return entities if entities else None\n",
    "\n",
    "# df[\"entities\"] = df[\"text\"].apply(lambda x: extract_entities(x) if isinstance(x, str) else [])\n",
    "# df = df.explode(\"entities\").dropna().reset_index(drop=True)  # Explode list of entities\n",
    "# print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Same function as above cell but instead prints out a single entity that is most frequent of the post.\n",
    "# import spacy\n",
    "# import pandas as pd\n",
    "# from collections import Counter\n",
    "\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def get_most_frequent_entity(text):\n",
    "    \"\"\"\n",
    "    Extracts entities (ORG, GPE, PRODUCT) and returns the most frequent one.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str) or not text:\n",
    "        return None\n",
    "\n",
    "    doc = nlp(text)\n",
    "    entities = [ent.text for ent in doc.ents if ent.label_ in [\"ORG\", \"GPE\", \"PRODUCT\"]]\n",
    "\n",
    "    if not entities:\n",
    "        return None\n",
    "\n",
    "    entity_counts = Counter(entities)\n",
    "    return entity_counts.most_common(1)[0][0]  # Return the most frequent entity\n",
    "\n",
    "# Example usage (assuming 'df' is your DataFrame):\n",
    "\n",
    "# df[\"most_frequent_entity\"] = df[\"text\"].apply(get_most_frequent_entity)\n",
    "# df = df.dropna(subset=['most_frequent_entity']).reset_index(drop=True)\n",
    "df[\"entity\"] = df[\"text\"].apply(get_most_frequent_entity)\n",
    "df = df.dropna(subset=['entity']).reset_index(drop=True)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns\n",
    "df.to_csv(\"df_w_entity.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def analyze_top_entities(df):\n",
    "    \"\"\"\n",
    "    Counts the frequency of entities in a DataFrame column and filters the DataFrame\n",
    "    to include only rows containing the top 5 most frequent entities.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing a column named 'entities'\n",
    "                                     (which should be a single entity per row).\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - list: A list of the top 5 most frequent entities.\n",
    "            - pd.DataFrame: A DataFrame filtered to include only rows where\n",
    "                              the 'entities' column contains one of the top 5 entities.\n",
    "    \"\"\"\n",
    "    # Count the frequency of each entity\n",
    "    top_entities = df[\"entity\"].value_counts().head(5).index.tolist()\n",
    "\n",
    "    # Filter dataset to only include these top entities\n",
    "    df_filtered = df[df[\"entity\"].isin(top_entities)].reset_index(drop=True)\n",
    "\n",
    "    print(\"Top 5 Entities:\", top_entities)\n",
    "    print(df_filtered.head())\n",
    "\n",
    "    return top_entities, df_filtered\n",
    "\n",
    "# Example usage (assuming 'df' is your DataFrame):\n",
    "\n",
    "top_entities_result, df_filtered_result = analyze_top_entities(df.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_filtered_result.to_csv(\"df_filtered.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_entity_frequency_over_time(df):\n",
    "    \"\"\"\n",
    "    Plots the frequency of entities over time (created_utc).\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame with columns 'entity' and 'created_utc'.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert 'created_utc' to datetime if it's not already\n",
    "    df['created_utc'] = pd.to_datetime(df['created_utc'], unit='s')\n",
    "\n",
    "    # Group by 'created_utc' and 'entity', then count occurrences\n",
    "    entity_time_counts = df.groupby([pd.Grouper(key='created_utc', freq='Y'), 'entity']).size().reset_index(name='count')\n",
    "    #D is daily, W Weekly, M Monthly\n",
    "\n",
    "    # Pivot the data for plotting\n",
    "    entity_time_pivot = entity_time_counts.pivot(index='created_utc', columns='entity', values='count').fillna(0)\n",
    "\n",
    "    # Plot the data\n",
    "    plt.figure(figsize=(15, 7))\n",
    "    for entity in entity_time_pivot.columns:\n",
    "        plt.plot(entity_time_pivot.index, entity_time_pivot[entity], label=entity)\n",
    "\n",
    "    plt.title('Entity Frequency Over Time')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.legend()\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_entity_frequency_over_time(df_filtered_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# def analyze_top_entities(df):\n",
    "#     \"\"\"\n",
    "#     Counts the frequency of entities in a DataFrame column and filters the DataFrame\n",
    "#     to include only rows containing the top 5 most frequent entities.\n",
    "\n",
    "#     Args:\n",
    "#         df (pd.DataFrame): DataFrame containing a column named 'entities'\n",
    "#                            (which should be a list of entities per row).\n",
    "\n",
    "#     Returns:\n",
    "#         tuple: A tuple containing:\n",
    "#             - list: A list of the top 5 most frequent entities.\n",
    "#             - pd.DataFrame: A DataFrame filtered to include only rows where\n",
    "#                             the 'entities' column contains one of the top 5 entities.\n",
    "#     \"\"\"\n",
    "#     # Explode the 'entities' column to count individual entity occurrences\n",
    "#     df_exploded = df.explode(\"entities\").dropna().reset_index(drop=True)\n",
    "\n",
    "#     # Count the frequency of each entity\n",
    "#     top_entities = df_exploded[\"entities\"].value_counts().head(5).index.tolist()\n",
    "\n",
    "#     # Filter dataset to only include these top entities\n",
    "#     df_filtered = df_exploded[df_exploded[\"entities\"].isin(top_entities)].reset_index(drop=True)\n",
    "\n",
    "#     print(\"Top 5 Entities:\", top_entities)\n",
    "#     print(df_filtered.head())\n",
    "\n",
    "#     return top_entities, df_filtered\n",
    "\n",
    "# top_entities_result, df_filtered_result = analyze_top_entities(df.copy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "def get_sentiment(text):\n",
    "    score = analyzer.polarity_scores(text)\n",
    "    return score[\"compound\"]\n",
    "\n",
    "df_filtered_result[\"sentiment\"] = df_filtered_result[\"text\"].apply(lambda x: get_sentiment(x) if isinstance(x, str) else 0)\n",
    "print(df_filtered_result.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_entity_sentiment_over_time(df):\n",
    "    \"\"\"\n",
    "    Plots the sentiment value of entities over time (created_utc).\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame with columns 'entity', 'created_utc', and 'sentiment'.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert 'created_utc' to datetime\n",
    "    df['created_utc'] = pd.to_datetime(df['created_utc'], unit='s')\n",
    "\n",
    "    # Group by time and entity, then calculate mean sentiment\n",
    "    entity_time_sentiment = df.groupby([pd.Grouper(key='created_utc', freq='D'), 'entity'])['sentiment'].mean().reset_index()\n",
    "\n",
    "    # Pivot the data for plotting\n",
    "    entity_time_pivot = entity_time_sentiment.pivot(index='created_utc', columns='entity', values='sentiment').fillna(0)\n",
    "\n",
    "    # Plot the data\n",
    "    plt.figure(figsize=(15, 7))\n",
    "    for entity in entity_time_pivot.columns:\n",
    "        plt.plot(entity_time_pivot.index, entity_time_pivot[entity], label=entity, marker='o', markersize=5)\n",
    "\n",
    "    plt.title('Entity Sentiment Over Time')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Sentiment')\n",
    "    plt.legend()\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Call the function\n",
    "plot_entity_sentiment_over_time(df_filtered_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
