{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import networkx as nx\n",
    "from collections import Counter\n",
    "import creds\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uncomment these to have full output on jupyter\n",
    "#pd.set_option('display.max_colwidth', None)\n",
    "#pd.set_option('display.max_rows', None)\n",
    "#pd.set_option('display.width', None)  # This one is important for terminal-like output\n",
    "#pd.set_option('display.expand_frame_repr', False) # Prevents line wrapping.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /root/anaconda3/lib/python3.11/site-packages (3.8.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /root/anaconda3/lib/python3.11/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /root/anaconda3/lib/python3.11/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /root/anaconda3/lib/python3.11/site-packages (from spacy) (1.0.12)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /root/anaconda3/lib/python3.11/site-packages (from spacy) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /root/anaconda3/lib/python3.11/site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /root/anaconda3/lib/python3.11/site-packages (from spacy) (8.3.4)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /root/anaconda3/lib/python3.11/site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /root/anaconda3/lib/python3.11/site-packages (from spacy) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /root/anaconda3/lib/python3.11/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /root/anaconda3/lib/python3.11/site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /root/anaconda3/lib/python3.11/site-packages (from spacy) (0.15.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /root/anaconda3/lib/python3.11/site-packages (from spacy) (4.65.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /root/anaconda3/lib/python3.11/site-packages (from spacy) (1.24.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /root/anaconda3/lib/python3.11/site-packages (from spacy) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /root/anaconda3/lib/python3.11/site-packages (from spacy) (1.10.8)\n",
      "Requirement already satisfied: jinja2 in /root/anaconda3/lib/python3.11/site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /root/anaconda3/lib/python3.11/site-packages (from spacy) (68.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /root/anaconda3/lib/python3.11/site-packages (from spacy) (23.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /root/anaconda3/lib/python3.11/site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: language-data>=1.2 in /root/anaconda3/lib/python3.11/site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /root/anaconda3/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /root/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /root/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /root/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /root/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.7.22)\n",
      "Requirement already satisfied: blis<1.3.0,>=1.2.0 in /root/anaconda3/lib/python3.11/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.2.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /root/anaconda3/lib/python3.11/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: click>=8.0.0 in /root/anaconda3/lib/python3.11/site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.0.4)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /root/anaconda3/lib/python3.11/site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /root/anaconda3/lib/python3.11/site-packages (from typer<1.0.0,>=0.3.0->spacy) (14.0.0)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /root/anaconda3/lib/python3.11/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /root/anaconda3/lib/python3.11/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (5.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /root/anaconda3/lib/python3.11/site-packages (from jinja2->spacy) (2.1.1)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in /root/anaconda3/lib/python3.11/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /root/anaconda3/lib/python3.11/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /root/anaconda3/lib/python3.11/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /root/anaconda3/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install spacy\n",
    "!{sys.executable} -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Main function to orchestrate the process.\"\"\"\n",
    "reddit = praw.Reddit(\n",
    "    client_id=creds.CLIENT_ID,\n",
    "    client_secret=creds.CLIENT_SECRET,\n",
    "    user_agent=creds.USER_AGENTS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching posts before: 2025-03-30 20:12:39.563271+00:00\n",
      "Fetching posts before: 2025-03-30 19:55:12+00:00\n",
      "Fetching posts before: 2025-03-30 19:55:12+00:00\n",
      "Fetching posts before: 2025-03-30 19:55:12+00:00\n",
      "Fetching posts before: 2025-03-30 19:55:12+00:00\n",
      "Fetching posts before: 2025-03-30 19:55:12+00:00\n",
      "Fetching posts before: 2025-03-30 19:55:12+00:00\n",
      "Fetching posts before: 2025-03-30 19:55:12+00:00\n",
      "Fetching posts before: 2025-03-30 19:55:12+00:00\n",
      "Fetching posts before: 2025-03-30 19:55:12+00:00\n",
      "Fetching posts before: 2025-03-30 19:55:12+00:00\n",
      "Fetching posts before: 2025-03-30 19:55:12+00:00\n",
      "Fetching posts before: 2025-03-30 19:55:12+00:00\n",
      "Fetching posts before: 2025-03-30 19:55:12+00:00\n",
      "Fetching posts before: 2025-03-30 19:55:12+00:00\n",
      "Fetching posts before: 2025-03-30 19:55:12+00:00\n",
      "Fetching posts before: 2025-03-30 19:55:12+00:00\n",
      "Fetching posts before: 2025-03-30 19:55:12+00:00\n",
      "Fetching posts before: 2025-03-30 19:55:12+00:00\n",
      "Fetching posts before: 2025-03-30 19:55:12+00:00\n",
      "Fetching posts before: 2025-03-30 19:55:12+00:00\n",
      "Fetching posts before: 2025-03-30 19:55:12+00:00\n",
      "Fetching posts before: 2025-03-30 19:55:12+00:00\n",
      "Fetching posts before: 2025-03-30 19:55:12+00:00\n",
      "Fetching posts before: 2025-03-30 19:55:12+00:00\n",
      "Fetching posts before: 2025-03-30 19:55:12+00:00\n",
      "Fetching posts before: 2025-03-30 19:55:12+00:00\n",
      "Fetching posts before: 2025-03-30 19:55:12+00:00\n",
      "Fetching posts before: 2025-03-30 19:55:12+00:00\n",
      "Fetching posts before: 2025-03-30 19:55:12+00:00\n",
      "Fetching posts before: 2025-03-30 19:55:12+00:00\n",
      "Fetching posts before: 2025-03-30 19:55:12+00:00\n",
      "Fetching posts before: 2025-03-30 19:55:12+00:00\n",
      "Fetching posts before: 2025-03-30 19:55:12+00:00\n",
      "Fetching posts before: 2025-03-30 19:55:12+00:00\n",
      "Fetching posts before: 2025-03-30 19:55:12+00:00\n",
      "Fetching posts before: 2025-03-30 19:55:12+00:00\n",
      "Fetching posts before: 2025-03-30 19:55:12+00:00\n",
      "Fetching posts before: 2025-03-30 19:55:12+00:00\n",
      "Fetching posts before: 2025-03-30 19:55:12+00:00\n",
      "Fetching posts before: 2025-03-30 19:55:12+00:00\n",
      "Fetching posts before: 2025-03-30 19:55:12+00:00\n",
      "Fetching posts before: 2025-03-30 19:55:12+00:00\n",
      "Fetching posts before: 2025-03-30 19:55:12+00:00\n",
      "Fetching posts before: 2025-03-30 19:55:12+00:00\n",
      "Fetching posts before: 2025-03-30 19:55:12+00:00\n",
      "Fetching posts before: 2025-03-30 19:55:12+00:00\n",
      "Fetching posts before: 2025-03-30 19:55:12+00:00\n",
      "Fetching posts before: 2025-03-30 19:55:12+00:00\n",
      "Fetching posts before: 2025-03-30 19:55:12+00:00\n",
      "Fetching posts before: 2025-03-30 19:55:12+00:00\n",
      "Fetching posts before: 2025-03-30 19:55:09+00:00\n",
      "Fetching posts before: 2025-03-30 19:55:12+00:00\n",
      "Fetching posts before: 2025-03-30 19:55:12+00:00\n",
      "Fetching posts before: 2025-03-30 19:55:12+00:00\n",
      "Fetching posts before: 2025-03-30 19:55:12+00:00\n",
      "Fetching posts before: 2025-03-30 19:55:09+00:00\n",
      "Fetching posts before: 2025-03-30 19:55:12+00:00\n",
      "Fetching posts before: 2025-03-30 19:55:12+00:00\n",
      "Fetching posts before: 2025-03-30 19:55:12+00:00\n",
      "Fetching posts before: 2025-03-30 19:55:09+00:00\n",
      "Fetching posts before: 2025-03-30 19:55:09+00:00\n",
      "Fetching posts before: 2025-03-30 19:55:12+00:00\n",
      "Fetching posts before: 2025-03-30 19:55:09+00:00\n",
      "Fetching posts before: 2025-03-30 19:55:12+00:00\n",
      "Fetching posts before: 2025-03-30 19:55:09+00:00\n",
      "Fetching posts before: 2025-03-30 19:55:09+00:00\n",
      "Fetching posts before: 2025-03-30 19:55:09+00:00\n",
      "Fetching posts before: 2025-03-30 19:55:09+00:00\n",
      "Fetching posts before: 2025-03-30 19:55:09+00:00\n",
      "Fetching posts before: 2025-03-30 19:55:09+00:00\n",
      "Fetching posts before: 2025-03-30 19:55:09+00:00\n",
      "Fetching posts before: 2025-03-30 19:55:09+00:00\n",
      "Fetching posts before: 2025-03-30 19:55:09+00:00\n",
      "Fetching posts before: 2025-03-30 19:55:09+00:00\n",
      "Fetching posts before: 2025-03-30 20:05:56+00:00\n",
      "Fetching posts before: 2025-03-30 20:05:56+00:00\n",
      "Fetching posts before: 2025-03-30 20:05:56+00:00\n",
      "Fetching posts before: 2025-03-30 20:05:56+00:00\n",
      "Fetching posts before: 2025-03-30 20:05:56+00:00\n",
      "Fetching posts before: 2025-03-30 20:05:56+00:00\n",
      "Fetching posts before: 2025-03-30 20:05:56+00:00\n",
      "Fetching posts before: 2025-03-30 20:05:56+00:00\n",
      "Fetching posts before: 2025-03-30 20:05:56+00:00\n",
      "Fetching posts before: 2025-03-30 20:05:56+00:00\n",
      "Fetching posts before: 2025-03-30 20:05:56+00:00\n",
      "Fetching posts before: 2025-03-30 20:05:56+00:00\n",
      "Fetching posts before: 2025-03-30 20:05:56+00:00\n",
      "Fetching posts before: 2025-03-30 20:05:56+00:00\n",
      "Fetching posts before: 2025-03-30 20:05:56+00:00\n",
      "Fetching posts before: 2025-03-30 20:05:56+00:00\n",
      "Fetching posts before: 2025-03-30 20:05:56+00:00\n",
      "Fetching posts before: 2025-03-30 20:05:56+00:00\n",
      "Fetching posts before: 2025-03-30 20:05:56+00:00\n",
      "Fetching posts before: 2025-03-30 20:05:56+00:00\n",
      "Fetching posts before: 2025-03-30 20:05:56+00:00\n",
      "Fetching posts before: 2025-03-30 20:05:56+00:00\n",
      "Fetching posts before: 2025-03-30 20:05:56+00:00\n",
      "Fetching posts before: 2025-03-30 20:05:56+00:00\n",
      "Fetching posts before: 2025-03-30 20:05:56+00:00\n",
      "DF Head---------------------------------\n",
      "  subreddit                                              title  \\\n",
      "0       all  Another one of my friendsshop “stuffed loveEis...   \n",
      "1       all                    EGR Port cleaning… is this bad?   \n",
      "2       all        Questions Thread (2025-03-31 to 2025-04-06)   \n",
      "3       all  I’m a this a midlife my wife is going through ...   \n",
      "4       all                                       Sevrer Rules   \n",
      "\n",
      "                                                text   created_utc  \\\n",
      "0                                                     1.743365e+09   \n",
      "1                 1998 f-150 4.6l V8 with 269k miles  1.743365e+09   \n",
      "2  This thread is for any and all basic gameplay ...  1.743365e+09   \n",
      "3  I unfortunately learned what it’s like to trul...  1.743365e+09   \n",
      "4  Rules:1 Yall Can Leak shit i dont care but gen...  1.743365e+09   \n",
      "\n",
      "         created_date  \n",
      "0 2025-03-30 20:01:13  \n",
      "1 2025-03-30 20:01:09  \n",
      "2 2025-03-30 20:01:00  \n",
      "3 2025-03-30 20:00:58  \n",
      "4 2025-03-30 20:00:56  \n",
      "DF Tail---------------------------------\n",
      "     subreddit                                              title  \\\n",
      "9995       all      Recs for a good hair salon - McAllen, Mission   \n",
      "9996       all         Natalie actually made me feel bad for Josh   \n",
      "9997       all                  Graveyard of Empires Sweden Guide   \n",
      "9998       all  King is a Great Person and an Average Sibling....   \n",
      "9999       all  I went NC with brother, sister, mother last ni...   \n",
      "\n",
      "                                                   text   created_utc  \\\n",
      "9995  I'm looking for a salon that cuts women's hair...  1.743365e+09   \n",
      "9996  Josh is soooo insufferable and manipulative an...  1.743365e+09   \n",
      "9997  Hey all. I want to try my hand at making a goo...  1.743365e+09   \n",
      "9998                                                     1.743365e+09   \n",
      "9999  Went NC last night via text with all living fa...  1.743365e+09   \n",
      "\n",
      "            created_date  \n",
      "9995 2025-03-30 20:06:09  \n",
      "9996 2025-03-30 20:06:02  \n",
      "9997 2025-03-30 20:05:58  \n",
      "9998 2025-03-30 20:05:57  \n",
      "9999 2025-03-30 20:05:56  \n",
      "Total posts fetched: 10000\n"
     ]
    }
   ],
   "source": [
    "# def fetch_reddit_posts(keywords, subreddits, days, limit):\n",
    "#     end_time = datetime.datetime.now(datetime.UTC)\n",
    "#     start_time = end_time - datetime.timedelta(days=days)\n",
    "\n",
    "#     posts = []\n",
    "#     for subreddit in subreddits:\n",
    "#         current_end_time = end_time.timestamp()\n",
    "#         while current_end_time > start_time.timestamp() and len(posts) < limit:\n",
    "#             print(f\"Fetching posts before: {datetime.datetime.fromtimestamp(current_end_time, datetime.UTC)}\")\n",
    "#             for submission in reddit.subreddit(subreddit).search(\n",
    "#                 \" OR \".join(keywords),\n",
    "#                 sort=\"new\",\n",
    "#                 limit=min(100, limit - len(posts)),  # Fetch up to 100 at a time, respecting the overall limit\n",
    "#                 params={'before': int(current_end_time)}  # Pass 'before' in the params dictionary\n",
    "#             ):\n",
    "#                 if submission.created_utc < start_time.timestamp():\n",
    "#                     break  # Stop if we've gone past the start time\n",
    "\n",
    "#                 posts.append({\n",
    "#                     \"subreddit\": subreddit,\n",
    "#                     \"title\": submission.title,\n",
    "#                     \"text\": submission.selftext,\n",
    "#                     \"created_utc\": submission.created_utc\n",
    "#                 })\n",
    "#             if posts:\n",
    "#                 current_end_time = posts[-1]['created_utc']\n",
    "#             else:\n",
    "#                 break # No more posts found for this time range\n",
    "\n",
    "#     return pd.DataFrame(posts)\n",
    "\n",
    "# keywords = [\"lazy\", \"lousy\", \"dirty\", \"bad\", \"terrible\", \"horrible\", \"unreliable\"]\n",
    "# subreddits = [\"all\"]\n",
    "# #subreddits = [\"airlines\", \"Comcast\", \"healthcare\", \"railways\"]\n",
    "# df = fetch_reddit_posts(keywords, subreddits, days=1000, limit=10000)\n",
    "# df[\"created_date\"] = pd.to_datetime(df[\"created_utc\"], unit=\"s\")\n",
    "# print(\"DF Head---------------------------------\")\n",
    "# print(df.head())\n",
    "# print(\"DF Tail---------------------------------\")\n",
    "# print(df.tail())\n",
    "# print(f\"Total posts fetched: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Same as above but limits post collection to one every 10 minute timespan\n",
    "def fetch_reddit_posts(keywords, subreddits, days, limit, time_interval_minutes):\n",
    "    \"\"\"\n",
    "    Fetches Reddit posts, ensuring no more than one post per specified timespan.\n",
    "\n",
    "    Args:\n",
    "        keywords (list): List of keywords to search for.\n",
    "        subreddits (list): List of subreddits to search in.\n",
    "        days (int): Number of days to search back.\n",
    "        limit (int): Maximum number of posts to retrieve.\n",
    "        time_interval_minutes (int, optional): Time interval in minutes. Defaults to 10.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing fetched posts.\n",
    "    \"\"\"\n",
    "\n",
    "    end_time = datetime.datetime.now(datetime.UTC)\n",
    "    start_time = end_time - datetime.timedelta(days=days)\n",
    "\n",
    "    posts = []\n",
    "    fetched_timestamps = set()  # Track timestamps to enforce interval rule\n",
    "    interval_seconds = time_interval_minutes * 60\n",
    "\n",
    "    for subreddit in subreddits:\n",
    "        current_end_time = end_time.timestamp()\n",
    "        while current_end_time > start_time.timestamp() and len(posts) < limit:\n",
    "            print(f\"Fetching posts before: {datetime.datetime.fromtimestamp(current_end_time, datetime.UTC)}\")\n",
    "            for submission in reddit.subreddit(subreddit).search(\n",
    "                \" OR \".join(keywords),\n",
    "                sort=\"new\",\n",
    "                limit=min(100, limit - len(posts)),\n",
    "                params={'before': int(current_end_time)}\n",
    "            ):\n",
    "                if submission.created_utc < start_time.timestamp():\n",
    "                    break  # Stop if we've gone past the start time\n",
    "\n",
    "                interval_group = int(submission.created_utc // interval_seconds)\n",
    "\n",
    "                if interval_group not in fetched_timestamps:\n",
    "                    posts.append({\n",
    "                        \"subreddit\": subreddit,\n",
    "                        \"title\": submission.title,\n",
    "                        \"text\": submission.selftext,\n",
    "                        \"created_utc\": submission.created_utc\n",
    "                    })\n",
    "                    fetched_timestamps.add(interval_group)\n",
    "\n",
    "                if posts:\n",
    "                    current_end_time = min(posts[-1]['created_utc'], current_end_time - 1)\n",
    "                else:\n",
    "                    break  # No more posts found for this time range\n",
    "\n",
    "    return pd.DataFrame(posts)\n",
    "\n",
    "keywords = [\"lazy\", \"lousy\", \"dirty\", \"bad\", \"terrible\", \"horrible\", \"unreliable\"]\n",
    "subreddits = [\"all\"]\n",
    "df = fetch_reddit_posts(keywords, subreddits, days=1000, limit=10000, time_interval_minutes=30) # time interval is now 30 minutes.\n",
    "df[\"created_date\"] = pd.to_datetime(df[\"created_utc\"], unit=\"s\")\n",
    "print(\"DF Head---------------------------------\")\n",
    "print(df.head())\n",
    "print(\"DF Tail---------------------------------\")\n",
    "print(df.tail())\n",
    "print(f\"Total posts fetched: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# def extract_entities(text):\n",
    "#     doc = nlp(text)\n",
    "#     entities = [ent.text for ent in doc.ents if ent.label_ in [\"ORG\", \"GPE\", \"PRODUCT\"]]\n",
    "#     return entities if entities else None\n",
    "\n",
    "# df[\"entities\"] = df[\"text\"].apply(lambda x: extract_entities(x) if isinstance(x, str) else [])\n",
    "# df = df.explode(\"entities\").dropna().reset_index(drop=True)  # Explode list of entities\n",
    "# print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 28\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m entity_counts\u001b[38;5;241m.\u001b[39mmost_common(\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# Return the most frequent entity\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Example usage (assuming 'df' is your DataFrame):\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# df[\"most_frequent_entity\"] = df[\"text\"].apply(get_most_frequent_entity)\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# df = df.dropna(subset=['most_frequent_entity']).reset_index(drop=True)\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mentity\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(get_most_frequent_entity)\n\u001b[1;32m     29\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mdropna(subset\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mentity\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(df\u001b[38;5;241m.\u001b[39mhead())\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/series.py:4760\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4625\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4626\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4627\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4632\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4633\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4634\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4635\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4636\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4751\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4752\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   4753\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SeriesApply(\n\u001b[1;32m   4754\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4755\u001b[0m         func,\n\u001b[1;32m   4756\u001b[0m         convert_dtype\u001b[38;5;241m=\u001b[39mconvert_dtype,\n\u001b[1;32m   4757\u001b[0m         by_row\u001b[38;5;241m=\u001b[39mby_row,\n\u001b[1;32m   4758\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[1;32m   4759\u001b[0m         kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[0;32m-> 4760\u001b[0m     )\u001b[38;5;241m.\u001b[39mapply()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/apply.py:1207\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1204\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[1;32m   1206\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[0;32m-> 1207\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_standard()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/apply.py:1287\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1281\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[1;32m   1282\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[1;32m   1283\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[1;32m   1284\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[1;32m   1285\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[1;32m   1286\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1287\u001b[0m mapped \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_map_values(\n\u001b[1;32m   1288\u001b[0m     mapper\u001b[38;5;241m=\u001b[39mcurried, na_action\u001b[38;5;241m=\u001b[39maction, convert\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_dtype\n\u001b[1;32m   1289\u001b[0m )\n\u001b[1;32m   1291\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1292\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1293\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1294\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[1;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[0;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m algorithms\u001b[38;5;241m.\u001b[39mmap_array(arr, mapper, na_action\u001b[38;5;241m=\u001b[39mna_action, convert\u001b[38;5;241m=\u001b[39mconvert)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/algorithms.py:1814\u001b[0m, in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1812\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1813\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1814\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer(values, mapper, convert\u001b[38;5;241m=\u001b[39mconvert)\n\u001b[1;32m   1815\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1816\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[1;32m   1817\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[1;32m   1818\u001b[0m     )\n",
      "File \u001b[0;32mlib.pyx:2917\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Cell \u001b[0;32mIn[7], line 15\u001b[0m, in \u001b[0;36mget_most_frequent_entity\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m text:\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m doc \u001b[38;5;241m=\u001b[39m nlp(text)\n\u001b[1;32m     16\u001b[0m entities \u001b[38;5;241m=\u001b[39m [ent\u001b[38;5;241m.\u001b[39mtext \u001b[38;5;28;01mfor\u001b[39;00m ent \u001b[38;5;129;01min\u001b[39;00m doc\u001b[38;5;241m.\u001b[39ments \u001b[38;5;28;01mif\u001b[39;00m ent\u001b[38;5;241m.\u001b[39mlabel_ \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mORG\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPE\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPRODUCT\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m entities:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/spacy/language.py:1040\u001b[0m, in \u001b[0;36mLanguage.__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m   1019\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m   1020\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1021\u001b[0m     text: Union[\u001b[38;5;28mstr\u001b[39m, Doc],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     component_cfg: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, Dict[\u001b[38;5;28mstr\u001b[39m, Any]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1025\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Doc:\n\u001b[1;32m   1026\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Apply the pipeline to some text. The text can span multiple sentences,\u001b[39;00m\n\u001b[1;32m   1027\u001b[0m \u001b[38;5;124;03m    and can contain arbitrary whitespace. Alignment into the original string\u001b[39;00m\n\u001b[1;32m   1028\u001b[0m \u001b[38;5;124;03m    is preserved.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1038\u001b[0m \u001b[38;5;124;03m    DOCS: https://spacy.io/api/language#call\u001b[39;00m\n\u001b[1;32m   1039\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1040\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_doc(text)\n\u001b[1;32m   1041\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m component_cfg \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1042\u001b[0m         component_cfg \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/spacy/language.py:1131\u001b[0m, in \u001b[0;36mLanguage._ensure_doc\u001b[0;34m(self, doc_like)\u001b[0m\n\u001b[1;32m   1129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m doc_like\n\u001b[1;32m   1130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(doc_like, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m-> 1131\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_doc(doc_like)\n\u001b[1;32m   1132\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(doc_like, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[1;32m   1133\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Doc(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab)\u001b[38;5;241m.\u001b[39mfrom_bytes(doc_like)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/spacy/language.py:1123\u001b[0m, in \u001b[0;36mLanguage.make_doc\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m   1119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(text) \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_length:\n\u001b[1;32m   1120\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1121\u001b[0m         Errors\u001b[38;5;241m.\u001b[39mE088\u001b[38;5;241m.\u001b[39mformat(length\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(text), max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_length)\n\u001b[1;32m   1122\u001b[0m     )\n\u001b[0;32m-> 1123\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer(text)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/spacy/tokenizer.pyx:161\u001b[0m, in \u001b[0;36mspacy.tokenizer.Tokenizer.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/spacy/tokenizer.pyx:264\u001b[0m, in \u001b[0;36mspacy.tokenizer.Tokenizer._apply_special_cases\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/spacy/tokenizer.pyx:304\u001b[0m, in \u001b[0;36mspacy.tokenizer.Tokenizer._prepare_special_spans\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/spacy/tokens/doc.pyx:500\u001b[0m, in \u001b[0;36mspacy.tokens.doc.Doc.__getitem__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/spacy/util.py:1282\u001b[0m, in \u001b[0;36mnormalize_slice\u001b[0;34m(length, start, stop, step)\u001b[0m\n\u001b[1;32m   1278\u001b[0m             new_excs[new_key] \u001b[38;5;241m=\u001b[39m new_value\n\u001b[1;32m   1279\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m new_excs\n\u001b[0;32m-> 1282\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnormalize_slice\u001b[39m(\n\u001b[1;32m   1283\u001b[0m     length: \u001b[38;5;28mint\u001b[39m, start: \u001b[38;5;28mint\u001b[39m, stop: \u001b[38;5;28mint\u001b[39m, step: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1284\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m]:\n\u001b[1;32m   1285\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (step \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m step \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m   1286\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE057)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Same function as above cell but instead prints out a single entity that is most frequent of the post.\n",
    "import spacy\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def get_most_frequent_entity(text):\n",
    "    \"\"\"\n",
    "    Extracts entities (ORG, GPE, PRODUCT) and returns the most frequent one.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str) or not text:\n",
    "        return None\n",
    "\n",
    "    doc = nlp(text)\n",
    "    entities = [ent.text for ent in doc.ents if ent.label_ in [\"ORG\", \"GPE\", \"PRODUCT\"]]\n",
    "\n",
    "    if not entities:\n",
    "        return None\n",
    "\n",
    "    entity_counts = Counter(entities)\n",
    "    return entity_counts.most_common(1)[0][0]  # Return the most frequent entity\n",
    "\n",
    "# Example usage (assuming 'df' is your DataFrame):\n",
    "\n",
    "# df[\"most_frequent_entity\"] = df[\"text\"].apply(get_most_frequent_entity)\n",
    "# df = df.dropna(subset=['most_frequent_entity']).reset_index(drop=True)\n",
    "df[\"entity\"] = df[\"text\"].apply(get_most_frequent_entity)\n",
    "df = df.dropna(subset=['entity']).reset_index(drop=True)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def analyze_top_entities(df):\n",
    "    \"\"\"\n",
    "    Counts the frequency of entities in a DataFrame column and filters the DataFrame\n",
    "    to include only rows containing the top 5 most frequent entities.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing a column named 'entities'\n",
    "                                     (which should be a single entity per row).\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - list: A list of the top 5 most frequent entities.\n",
    "            - pd.DataFrame: A DataFrame filtered to include only rows where\n",
    "                              the 'entities' column contains one of the top 5 entities.\n",
    "    \"\"\"\n",
    "    # Count the frequency of each entity\n",
    "    top_entities = df[\"entity\"].value_counts().head(5).index.tolist()\n",
    "\n",
    "    # Filter dataset to only include these top entities\n",
    "    df_filtered = df[df[\"entity\"].isin(top_entities)].reset_index(drop=True)\n",
    "\n",
    "    print(\"Top 5 Entities:\", top_entities)\n",
    "    print(df_filtered.head())\n",
    "\n",
    "    return top_entities, df_filtered\n",
    "\n",
    "# Example usage (assuming 'df' is your DataFrame):\n",
    "\n",
    "top_entities_result, df_filtered_result = analyze_top_entities(df.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_entity_frequency_over_time(df):\n",
    "    \"\"\"\n",
    "    Plots the frequency of entities over time (created_utc).\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame with columns 'entity' and 'created_utc'.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert 'created_utc' to datetime if it's not already\n",
    "    df['created_utc'] = pd.to_datetime(df['created_utc'], unit='s')\n",
    "\n",
    "    # Group by 'created_utc' and 'entity', then count occurrences\n",
    "    entity_time_counts = df.groupby([pd.Grouper(key='created_utc', freq='Y'), 'entity']).size().reset_index(name='count')\n",
    "    #D is daily, W Weekly, M Monthly\n",
    "\n",
    "    # Pivot the data for plotting\n",
    "    entity_time_pivot = entity_time_counts.pivot(index='created_utc', columns='entity', values='count').fillna(0)\n",
    "\n",
    "    # Plot the data\n",
    "    plt.figure(figsize=(15, 7))\n",
    "    for entity in entity_time_pivot.columns:\n",
    "        plt.plot(entity_time_pivot.index, entity_time_pivot[entity], label=entity)\n",
    "\n",
    "    plt.title('Entity Frequency Over Time')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.legend()\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_entity_frequency_over_time(df_filtered_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# def analyze_top_entities(df):\n",
    "#     \"\"\"\n",
    "#     Counts the frequency of entities in a DataFrame column and filters the DataFrame\n",
    "#     to include only rows containing the top 5 most frequent entities.\n",
    "\n",
    "#     Args:\n",
    "#         df (pd.DataFrame): DataFrame containing a column named 'entities'\n",
    "#                            (which should be a list of entities per row).\n",
    "\n",
    "#     Returns:\n",
    "#         tuple: A tuple containing:\n",
    "#             - list: A list of the top 5 most frequent entities.\n",
    "#             - pd.DataFrame: A DataFrame filtered to include only rows where\n",
    "#                             the 'entities' column contains one of the top 5 entities.\n",
    "#     \"\"\"\n",
    "#     # Explode the 'entities' column to count individual entity occurrences\n",
    "#     df_exploded = df.explode(\"entities\").dropna().reset_index(drop=True)\n",
    "\n",
    "#     # Count the frequency of each entity\n",
    "#     top_entities = df_exploded[\"entities\"].value_counts().head(5).index.tolist()\n",
    "\n",
    "#     # Filter dataset to only include these top entities\n",
    "#     df_filtered = df_exploded[df_exploded[\"entities\"].isin(top_entities)].reset_index(drop=True)\n",
    "\n",
    "#     print(\"Top 5 Entities:\", top_entities)\n",
    "#     print(df_filtered.head())\n",
    "\n",
    "#     return top_entities, df_filtered\n",
    "\n",
    "# top_entities_result, df_filtered_result = analyze_top_entities(df.copy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "def get_sentiment(text):\n",
    "    score = analyzer.polarity_scores(text)\n",
    "    return score[\"compound\"]\n",
    "\n",
    "df_filtered_result[\"sentiment\"] = df_filtered_result[\"text\"].apply(lambda x: get_sentiment(x) if isinstance(x, str) else 0)\n",
    "print(df_filtered_result.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_entity_sentiment_over_time(df):\n",
    "    \"\"\"\n",
    "    Plots the sentiment value of entities over time (created_utc).\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame with columns 'entity', 'created_utc', and 'sentiment'.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert 'created_utc' to datetime\n",
    "    df['created_utc'] = pd.to_datetime(df['created_utc'], unit='s')\n",
    "\n",
    "    # Group by time and entity, then calculate mean sentiment\n",
    "    entity_time_sentiment = df.groupby([pd.Grouper(key='created_utc', freq='D'), 'entity'])['sentiment'].mean().reset_index()\n",
    "\n",
    "    # Pivot the data for plotting\n",
    "    entity_time_pivot = entity_time_sentiment.pivot(index='created_utc', columns='entity', values='sentiment').fillna(0)\n",
    "\n",
    "    # Plot the data\n",
    "    plt.figure(figsize=(15, 7))\n",
    "    for entity in entity_time_pivot.columns:\n",
    "        plt.plot(entity_time_pivot.index, entity_time_pivot[entity], label=entity, marker='o', markersize=5)\n",
    "\n",
    "    plt.title('Entity Sentiment Over Time')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Sentiment')\n",
    "    plt.legend()\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Call the function\n",
    "plot_entity_sentiment_over_time(df_filtered_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
